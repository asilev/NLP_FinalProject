\documentclass[a4paper]{article}
\usepackage{graphicx}
\author{Lev, Asi\\
	\texttt{ID 039833900}\\
	\texttt{email: asi.lev@gmail.com}
\and
	Jashek, Ronen\\
	\texttt{ID 025340993}\\
	\texttt{email: ronen.jashek@gmail.com}	
}

\title{A Method for Hebrew Stylometry, based on Short Messages}
\date{}
\begin{document}
\maketitle
%\begin{center}
\begin{abstract}
\textbf{@asi}\\
Given a set of short text messages in Hebrew, we attempt to classify each sentence to it respective author, according to its style of writing.
We use Tweeter as our data set, and we've checked X users with approximately X tweets per user to train our model.
We then use X tweets per user in order to evaluate our model's accuracy.
% Continue with abstract results
\end{abstract}
%\end{center}
\section{Introduction}
\textbf{@asi}\\
Stylometry is the practice being used to identify an author based on his style of writing.
While stylometry has been used historically to identify authors of long texts such as novels and plays, legacy practices were proven inefficient when applied to short text messages such as SMS\footnote{Short Message Service} and social media posts.
Recently, more novel approaches were suggested for stylometry that utilize algorithms in machine learning and deep learning, to improve accuracy of stylometry on short texts.
These works rely on several learning models, such as Bayesian statistics \emph{(INSERT CITATION)}, Support Vector Machine \emph{(INSERT CITATION)}, Neural Networks \emph{(INSERT CITATION)}, and others.
These works also suggest different approaches to features selection. The study of feature selection may not be suitable to any language, as different languages may hold different features that may be suitable to a more accurate system.
In this work we started an initial research and benchmarking on the influence of different learning models and features, in order to learn more about which of these models and features influence on the accuracy of a possible stylometry system for short messages in Hebrew.
\section{Article Structure}
\textbf{@asi}\\
\emph{Optionally
Describe the structure of this document according to sections
Check formal name for this section
}
This document is arranged according to the following order:
We begin by describing related work this article relies on in section \ref{Related Work}.
We then proceed by describing the architecture of our utility, that implements a framework for model benchmarking in section \ref{Architecture}.
After that, we describe our approach of research and benchmarking in section \ref{Approach}.
We then describe our results in section \ref{Results}.
Further work is suggested in section \ref{Further Work}.
Finally, we conclude our work in section \ref{Conclusion}.
\section{Related Work}
\label{Related Work}
\textbf{@ronen}\\
There has been significant amount of work and progress achieved in the past 10-15 years in the field of author classification and identification based texts, initially ranging from a few paragraphs to complete books. As the usage of emails in general, and social media in particular, became increasingly popular, significant amounts of work were invested in shorter texts.
On longer texts, usually satisfactory-high results were achieved. Qian, He and Zhang [2017, \cite{qian}] showed that deep learning models on authorship identification and authorship verification can yield an accuracy of 69.1\% on Reuters\_50\_50 (Reuters data-set) and 89.2\% on Gutenberg data-set (a data-set established by the authors), while Siamese network achieved an verification accuracy of 99.8\% on both Reuters\_50\_50 and Gutenberg data-sets.\\
However, despite the vast amount of work invested, it is still challenging to achieve reasonably-high detection results on messages derived from the aforementioned social media (Twitter, Facebook, etc.), emails and such, especially as the number of authors increases; for a small set of authors (sometimes as low as 2) good results can be achieved. Green and Sheppard \cite{rachel} experimented with both BOW and Style Markers, and showed that SM is superior (average classification accuracy of 92.3\% vs. 76.7\%). However, this high classification rate was achieved on 2 authors, whereas increasing from 2 to 5 authors dropped the accuracy quickly (around 55\%) up to the lowest accuracy of 40\% reached on 12 authors.
Schwartz, Tsur, Rappoport and Koppel \cite{schwartz} experimented (among other criteria) with varying training set sizes and number of authors. They demonstrated an improvement in accuracy from 49.5\% (50 tweets per 50 authors) to 69.7\% (larger amount of tweets per 50 authors), but with a large number of authors (1000) showed 30.3\% accuracy (using 200 tweets/author), which correlates to the trend shown in other works as well. Albeit, 30.3\% is still  distinguishably better than the random baseline of 0.1\%.
This trend is further validated in the work of Brocardo, Traore, Saad and Woungang \cite{brocardo}, albeit their work was founded on emails from the Enron data-set (200K emails from 150 users, average number of words per email is 200). Their techniques were based on a combination of supervised learning and n-gram analysis. Even after applying a few pre-processing operations on the text (e.g. e-mails were combined to produce a single long message per individual, and then divided into smaller blocks used for authorship verification), their experiments yielded an EER (an Equal Error Rate metric used by the authors which corresponds to the operating point where False Acceptance and False Rejection rates have the same value) of 14.35\% for 87 users for relatively small block sizes.\\
It is important to notice that thus far the related works described herein are mostly involved with messages in the English language. In an overall perspective, little work was done on Semitic languages such as Hebrew and Arabic. The work done by Rabab’ah, Al-Ayyoub, Jararweh and Aldwairi \cite{rabab} on tweets in Arabic showed the best accuracy when employing SVM technique on a unified collection of features (more than 900) using tweets from 12 authros, usually around 3000 tweets per author. The best accuracy achieved was 68.67\%.\\
An excellent and comprehensive overview of modern authorship attribution methods can be found in \cite{stamatatos}.



\section{Architecture}
\label{Architecture}
\textbf{@asi}\\

Figure \ref{fig:architecture} illustrates the architecture of the benchmarking platform we've built for this work.\\
\begin{figure}[hb]
	\includegraphics[width=1\textwidth]{"architecture/NLP Author Classification Architecture"}
	\caption{Platform Architecture}
		\label{fig:architecture}
\end{figure}\\

We use "Data Source" as the abstract utility to pull text from short messages source, and save the messages with the correct tagged author for each short message in a well formed structure.
In here we've implemented the "Data Source" with a "Tweeter Data source", that pulls information according to parameters such as time period and a list of users from tweeter.
This data is then being processed by "Data Normalization" utility, that reads this data, filters unwanted cases (as will be described in section \ref{Approach}), and transforms the data to a structure that can be used by YAP \emph{CITE}.
We then use YAP to provide initial parsing for our data, and provide more information about it, such as morphological segmentation, stemming, POS tagging, etc.\
Then, we choose our features according to some model. This component takes the list of sentences, and builds for each sentence a list of features, where each feature is assigned with a value that describes that feature for the particular sentence.
We create two lists of sentences: one for the training (gold list) and one for the evaluation (test list).
Then, we use some learning model to train on the gold list, and produce the parameters that will be used for the encoder.
Lastly, we use the encoder on the test list, and compare its results with the actual authors from the test list using the evaluation component, to test the system's accuracy (hit rate).

\section{Approach}
\label{Approach}
\textbf{@asi}\\
We now describe the benchmarking approach step by step, and describe how each step is implemented in our work.
\subsection{Data Acquisition}
\textbf{@ronen}\\
\subsection{Data Exploration}
\textbf{@ronen}\\
\subsection{Data Preparation}
\textbf{@ronen}\\
Focus on Hebrew such as morphemes separation
Comment: credit as required by forum
\subsection{Feature Selection and Benchmarking}
\textbf{@?}\\
There are several approaches to features selection. In \cite{rachel} two approaches, namely bag of words vs.\ style markers.
In bag of words, each feature is a word, and the associated value is the number of time this word appears on the sentence.
In Style Marker, each feature is a selected feature of the sentence. It's value can be any numeric classifier for the feature, such as exist (1) or non exist (0), number of occurrences, or probability (percentage of occurrences).
While testing each of these approaches ourselves, we suggest a more generalized approach that also tests combinations of these approaches, by using some features as words from bag of words model, and some \emph{additional} features as style markers.
We also tested different approaches, such as N-Grams and POS analysis. The list features we used is:
\begin{itemize}
\item \textbf{\emph{Simple Word (Bag of Words)}}: Each feature is a word, and the value is the number of times the word appears in the sentence.
\item \textbf{\emph{Stemmed Word}}: Each feature is the stem analysis of a word, and the value is the number of times the stemmed word appears in the sentence.
\item \textbf{\emph{Bigram}}: Each feature is a sequence of two words, and the value is the number of times this pair of words appears in the sentence. We can further generalize this feature to any N-gram, but for a small corpus, this is proven as ineffective even for a bigram, since specific pair are very unlikely to reappear.
\item \textbf{\emph{POS Analysis}}: Each feature is a word, combined with its proper POS tag. This helps separate different words that may be written the same way, but used for different meanings.
\item \textbf{\emph{Number of Morphemes}}: A single feature, describing the number of morphemes the sentence holds.
\item \textbf{\emph{Average Word Size}}: A single feature, describing the average size of a word in the sentence.
\item \textbf{\emph{Number of Characters}}: A single feature, describing the number of characters in the sentence.
\item \textbf{\emph{Number of Punctuation Marks}}: a single value, describing the number of punctuation marks used in the sentence.
\item \textbf{\emph{Average Number of Punctuation Marks}}: A single value, describing the number of morphemes that represent punctuation marks, relative to the total number of morphemes in the sentence.
\item \textbf{\emph{POS Usage}}: Each feature is a POS, and the value is the number of times the POS appears in the sentence.
\item \textbf{\emph{Average Sentence Size}}: If the given sentence is divided to more than one actual sentence using a dot, this feature holds the average number of morphemes in each actual sentence in the complete given sentence.
\item \textbf{\emph{Number of Long Words}}: Each feature represents the number of words in the sentences, that are longer than a given threshold.
\end{itemize}
Since using many features slows down any learning model, and some of the features approaches are inherently large (such as BOW, N-Grams, etc.), we started by testing each feature approach separately to see its strength individually. We then tried combination of strong feature approaches and tested the contribution of the new features to the list of already decided features.
\emph{Here we need to describe each step, and show some results}

\subsection{Model Selection and Benchmarking}
\textbf{@asi}\\
In this work we used two machine learning models: Logistic Regression and Support Vector Machine.
In our platform, Logistic Regression model was implemented from scratch, as it is a very basic model, and for SVM we've used \emph{CITE}.
Comparing Logistic Regression and SVM for single feature at a time and carefully searching for optimal hyper parameters for both models provided similar results, but with much slower training time for Logistic Regression.
Adding more features to the model further slows down Logistic Regression more extremely than SVM.
Since we've seen the the Logistic Regression model (at least our implementation of it) was extremely slow to train, and results were not improved comparing to SVM, we chose to use SVM as the model for further investigations.
Our work did not test Neural Networks model, and the more novel Convolutional Neural Networks / Recurrent Neural Networks, which might provide better results, as shown in \emph{CITE}. This, together with more in depth research on other models, may be a good further work on the subject.

While we see our features can be highly unscaled, we've seen that scaling our features always produces a significant degradation in accuracy.
We therefore decided not to scale features before training the model.
Hyper parameters where chosen using a grid search with logarithmic scale.
\section{Results}
\label{Results}
\section{Further Work}
\label{Further Work}
possible under discussion, if more subsections required
\section{Conclusion}
\label{Conclusion}
\section{Acknowledgments}
\label{Acknowledgments}
\textbf{@ronen}\\
Most probably this is the section to cite and thank YAP, SVM, etc.


\begin{thebibliography}{9}

\bibitem{rachel}
	Rachel M. Green, John W. Sheppard
	\textit{Comparing Frequency and Style based Features for Twitter Author Identification}
\bibitem{qian}
	Chen Qian, Tianchang He, Rao Zhang
	\textit{Deep Learning based Authorship Identification}
\bibitem{schwartz}
	Roy Schwartz, Oren Tsur, Ari Rappoport, Moshe Koppel
	\textit{Authorship Attribution of Micro-Messages}
\bibitem{brocardo}
	Marcelo Luiz Brocardo, Issa Traore, Sherif Saad, Isaac Woungang
	\textit{Authorship Verification for Short Messages using Stylometry}
\bibitem{rabab}
	Abdullateef Rabab’ah, Mahmoud Al-Ayyoub, Yaser Jararweh, Monther Aldwairi
	\textit{Authorship Attribution of Arabic Tweets}
\bibitem{stamatatos}
	Efstathios Stamatatos
	\textit{A Survey of Modern Author ship Attribution Methods}
	
\end{thebibliography}
\end{document}