\documentclass[a4paper]{article}
\usepackage{graphicx}
\author{Lev, Asi\\
	\texttt{ID 039833900}\\
	\texttt{email: asi.lev@gmail.com}
\and
	Jashek, Ronen\\
	\texttt{ID 025340993}\\
	\texttt{email: ronenjashek@gmail.com}	
}

\title{A Method for Hebrew Stylometry, based on Short Messages}
\date{}
\begin{document}
\maketitle
%\begin{center}
\begin{abstract}
\textbf{@asi}\\
Given a set of short text messages in Hebrew, we attempt to classify each sentence to it respective author, according to its style of writing.
We use Tweeter as our data set, and we've checked X users with approximately X tweets per user to train our model.
We then use X tweets per user in order to evaluate our model's accuracy.
% Continue with abstract results
\end{abstract}
%\end{center}
\section{Introduction}
\textbf{@asi}\\
Stylometry is the practice being used to identify an author based on his style of writing.
While stylometry has been used historically to identify authors of long texts such as novels and plays, legacy practices were proven inefficient when applied to short text messages such as SMS\footnote{Short Message Service} and social media posts.
Recently, more novel approaches were suggested for stylometry that utilize algorithms in machine learning and deep learning, to improve accuracy of stylometry on short texts.
These works rely on several learning models, such as Bayesian statistics \emph{(INSERT CITATION)}, Support Vector Machine \emph{(INSERT CITATION)}, Neural Networks \emph{(INSERT CITATION)}, and others.
These works also suggest different approaches to features selection. The study of feature selection may not be suitable to any language, as different languages may hold different features that may be suitable to a more accurate system.
In this work we started an initial research and benchmarking on the influence of different learning models and features, in order to learn more about which of these models and features influence on the accuracy of a possible stylometry system for short messages in Hebrew.
\section{Article Structure}
\textbf{@asi}\\
\emph{Optionally
Describe the structure of this document according to sections
Check formal name for this section
}
This document is arrange according to the following order:
We begin by describing related work this article relies on in section \ref{Related Work}.
We then proceed by describing the architecture of our utility, that implements a framework for model benchmarking in section \ref{Architecture}.
After that, we describe our approach of research and benchmarking in section \ref{Approach}.
We then describe our results in section \ref{Results}.
Further work is suggested in section \ref{Further Work}.
Finally, we conclude our work in section \ref{Conclusion}.
\section{Related Work}
\label{Related Work}
\textbf{@ronen}\\
\section{Architecture}
\label{Architecture}
\textbf{@asi}\\
\begin{figure}
	\includegraphics[width=1\textwidth]{"architecture/NLP Author Classification Architecture"}
	\caption{Platform Architecture.}
		\label{fig:architecture}
\end{figure}
Figure \ref{fig:architecture} illustrates the architecture of the benchmarking platform we've built for this work.
We use "Data Source" as the abstract utility to pull text from short messages source, and save the messages with the correct tagged author for each short message in a well formed structure.
In here we've implemented the "Data Source" with a "Tweeter Data source", that pulls information according to parameters such as time period and a list of users from tweeter.
This data is then being processed by "Data Normalization" utility, that reads this data, filters unwanted cases (as will be described in section \ref{Approach}), and transforms the data to a structure that can be used by YAP \emph{CITE}.
We then use YAP to provide initial parsing for our data, and provide more information about it, such as morphological segmentation, stemming, POS tagging, etc.\
Then, we choose our features according to some model. This component takes the list of sentences, and builds for each sentence a list of features, where each feature is assigned with a value that describes that feature for the particular sentence.
We create two lists of sentences: one for the training (gold list) and one for the evaluation (test list).
Then, we use some learning model to train on the gold list, and produce the parameters that will be used for the encoder.
Lastly, we use the encoder on the test list, and compare its results with the actual authors from the test list using the evaluation component, to test the system's accuracy (hit rate).
\section{Approach}
\label{Approach}
\textbf{@asi}\\
We now describe the benchmarking approach step by step, and describe how each step is implemented in our work.
\subsection{Data Acquisition}
\textbf{@ronen}\\
\subsection{Data Exploration}
\textbf{@ronen}\\
\subsection{Data Preparation}
\textbf{@ronen}\\
Focus on Hebrew such as morphemes separation
Comment: credit as required by forum
\subsection{Feature Selection and Benchmarking}
\textbf{@?}\\
There are several approaches to features selection. In \cite{rachel} two approaches, namely bag of words vs.\ style markers.
In bag of words, each feature is a word, and the associated value is the number of time this word appears on the sentence.
In Style Marker, each feature is a selected feature of the sentence. It's value can be any numeric classifier for the feature, such as exist (1) or non exist (0), number of occurrences, or probability (percentage of occurrences).
While testing each of these approaches ourselves, we suggest a more generalized approach that also tests combinations of these approaches, by using some features as words from bag of words model, and some \emph{additional} features as style markers.
We also tested different approaches, such as N-Grams and POS analysis. The list features we used is:
\begin{itemize}
\item \textbf{\emph{Simple Word (Bag of Words)}}: Each feature is a word, and the value is the number of times the word appears in the sentence.
\item \textbf{\emph{Stemmed Word}}: Each feature is the stem analysis of a word, and the value is the number of times the stemmed word appears in the sentence.
\item \textbf{\emph{Bigram}}: Each feature is a sequence of two words, and the value is the number of times this pair of words appears in the sentence. We can further generalize this feature to any N-gram, but for a small corpus, this is proven as ineffective even for a bigram, since specific pair are very unlikely to reappear.
\item \textbf{\emph{POS Analysis}}: Each feature is a word, combined with its proper POS tag. This helps separate different words that may be written the same way, but used for different meanings.
\item \textbf{\emph{Number of Morphemes}}: A single feature, describing the number of morphemes the sentence holds.
\item \textbf{\emph{Average Word Size}}: A single feature, describing the average size of a word in the sentence.
\item \textbf{\emph{Number of Characters}}: A single feature, describing the number of characters in the sentence.
\item \textbf{\emph{Number of Punctuation Marks}}: a single value, describing the number of punctuation marks used in the sentence.
\item \textbf{\emph{Average Number of Punctuation Marks}}: A single value, describing the number of morphemes that represent punctuation marks, relative to the total number of morphemes in the sentence.
\item \textbf{\emph{POS Usage}}: Each feature is a POS, and the value is the number of times the POS appears in the sentence.
\item \textbf{\emph{Average Sentence Size}}: If the given sentence is divided to more than one actual sentence using a dot, this feature holds the average number of morphemes in each actual sentence in the complete given sentence.
\item \textbf{\emph{Number of Long Words}}: Each feature represents the number of words in the sentences, that are longer than a given threshold.
\end{itemize}
Since using many features slows down any learning model, and some of the features approaches are inherently large (such as BOW, N-Grams, etc.), we started by testing each feature approach separately to see its strength individually. We then tried combination of strong feature approaches and tested the contribution of the new features to the list of already decided features.
\emph{Here we need to describe each step, and show some results}

\subsection{Model Selection and Benchmarking}
\textbf{@asi}\\
In this work we used two machine learning models: Logistic Regression and Support Vector Machine.
In our platform, Logistic Regression model was implemented from scratch, as it is a very basic model, and for SVM we've used \emph{CITE}.
Comparing Logistic Regression and SVM for single feature at a time and carefully searching for optimal hyper parameters for both models provided similar results, but with much slower training time for Logistic Regression.
Adding more features to the model further slows down Logistic Regression more extremely than SVM.
Since we've seen the the Logistic Regression model (at least our implementation of it) was extremely slow to train, and results were not improved comparing to SVM, we chose to use SVM as the model for further investigations.

Our work did not test Neural Networks model, and the more novel Convolutional Neural Networks / Recurrent Neural Networks, which might provide better results, as shown in \emph{CITE}.
While we see our features can be highly unscaled, we've seen that scaling our features always produces a significant degradation in accuracy.
We therefore decided not to scale features before training the model.
Hyper parameters where chosen using a grid search with logarithmic scale.
\section{Results}
\label{Results}
\section{Further Work}
\label{Further Work}
possible under discussion, if more subsections required
\section{Conclusion}
\label{Conclusion}

\begin{thebibliography}{9}

\bibitem{rachel}
	Rachel M. Green, John W. Sheppard
	\textit{Comparing Frequency and Style based Features for Twitter Author Identification}.
\end{thebibliography}
\end{document}